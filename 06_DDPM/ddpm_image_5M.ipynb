{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e3357b-a937-4f55-8ea8-c541e79bcae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from loguru import logger\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#####################\n",
    "## UNet Components ##\n",
    "#####################\n",
    "\n",
    "from ddpm_components import DoubleConv, Down, SelfAttention, Up\n",
    "\n",
    "###############\n",
    "## Utilities ##\n",
    "###############\n",
    "\n",
    "from utils import get_data, create_diffusion_animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6247eb-14ca-41c6-b9ca-d2c9a122d239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 5897155\n"
     ]
    }
   ],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, time_dim=256, device=device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        # Initial Conv (no size change)\n",
    "        self.initial_conv = DoubleConv(in_channels, 32)\n",
    "        \n",
    "        # Encoder (Down) - each Down has MaxPool2d that halves spatial size\n",
    "        self.down1 = Down(32, 64)              # 64 -> 32\n",
    "        self.sa1 = SelfAttention(64, 32)       # spatial size: 32x32\n",
    "        self.down2 = Down(64, 128)             # 32 -> 16\n",
    "        self.sa2 = SelfAttention(128, 16)      # spatial size: 16x16\n",
    "        self.down3 = Down(128, 128)            # 16 -> 8\n",
    "        self.sa3 = SelfAttention(128, 8)       # spatial size: 8x8\n",
    "        \n",
    "        # Bottle-neck (no size change)\n",
    "        self.bot1 = DoubleConv(128, 256)       # spatial size: 8x8\n",
    "        self.bot2 = DoubleConv(256, 256)       # spatial size: 8x8\n",
    "        self.bot3 = DoubleConv(256, 128)       # spatial size: 8x8\n",
    "        \n",
    "        # Decoder (Up) - each Up has Upsample that doubles spatial size\n",
    "        self.up1 = Up(256, 64)                 # 8 -> 16 (concat with x3: 128ch)\n",
    "        self.sa4 = SelfAttention(64, 16)       # spatial size: 16x16\n",
    "        self.up2 = Up(128, 32)                 # 16 -> 32 (concat with x2: 64ch)\n",
    "        self.sa5 = SelfAttention(32, 32)       # spatial size: 32x32\n",
    "        self.up3 = Up(64, 32)                  # 32 -> 64 (concat with x1: 32ch)\n",
    "        self.sa6 = SelfAttention(32, 64)       # spatial size: 64x64\n",
    "        \n",
    "        # Out Conv\n",
    "        self.out_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "        \n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1. / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "        \n",
    "        # Encoder (Down path)\n",
    "        x1 = self.initial_conv(x)      # 64x64, 32 ch\n",
    "        x2 = self.down1(x1, t)         # 32x32, 64 ch\n",
    "        x2 = self.sa1(x2)              # 32x32, 64 ch\n",
    "        x3 = self.down2(x2, t)         # 16x16, 128 ch\n",
    "        x3 = self.sa2(x3)              # 16x16, 128 ch\n",
    "        x4 = self.down3(x3, t)         # 8x8, 128 ch\n",
    "        x4 = self.sa3(x4)              # 8x8, 128 ch\n",
    "        \n",
    "        # Bottle-neck\n",
    "        x4 = self.bot1(x4)             # 8x8, 256 ch\n",
    "        x4 = self.bot2(x4)             # 8x8, 256 ch\n",
    "        x4 = self.bot3(x4)             # 8x8, 128 ch\n",
    "        \n",
    "        # Decoder (Up path)\n",
    "        x = self.up1(x4, x3, t)        # 16x16, 64 ch\n",
    "        x = self.sa4(x)                # 16x16, 64 ch\n",
    "        x = self.up2(x, x2, t)         # 32x32, 32 ch\n",
    "        x = self.sa5(x)                # 32x32, 32 ch\n",
    "        x = self.up3(x, x1, t)         # 64x64, 32 ch\n",
    "        x = self.sa6(x)                # 64x64, 32 ch\n",
    "        out = self.out_conv(x)         # 64x64, 3 ch\n",
    "        return out\n",
    "\n",
    "model = UNet()\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "666178b0-3618-470d-b40c-76f0a0aabf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "## Flower Images ##\n",
    "###################\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "from datasets import load_dataset\n",
    "flowers = load_dataset(path=\"nkirschi/oxford-flowers\", split=\"test\")\n",
    "flowers = flowers['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "674137a1-9501-4fc7-8ff4-693bb2032b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317693b1c59d4f7eae63dc6ef032d9a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/4000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "Epoch: 1 | Loss: 0.34301681417752716 | Current LR: 0.0002\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd3a376c6f942d7bf9d80aec573b43a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sampling: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'create_diffusion_animation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m#-------\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m## Train\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#-------\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mddpm_components\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflowers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Long_AISDL/DeepLearning_PyTorch/06_DDPM/ddpm_components.py:302\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, data, epochs, lr, img_size, batch_size, visualize, report_interval)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize == \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    301\u001b[39m     _, img_list = diffusion.sample(model, n=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     fig = \u001b[43mcreate_diffusion_animation\u001b[49m(img_list)\n\u001b[32m    303\u001b[39m     fig.show()\n",
      "\u001b[31mNameError\u001b[39m: name 'create_diffusion_animation' is not defined"
     ]
    }
   ],
   "source": [
    "####################\n",
    "## Trainning loop ##\n",
    "####################\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 6\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = UNet().to(device=device)\n",
    "\n",
    "#-------\n",
    "## Train\n",
    "#-------\n",
    "\n",
    "from ddpm_components import train\n",
    "\n",
    "train(model=model, data=flowers, epochs=4000, img_size=IMG_SIZE, batch_size=BATCH_SIZE, report_interval=1000, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579e4ad-16db-4bcb-9504-91477eaaf87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df29a944-ca02-44b2-b36a-09a9b0e5b560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
